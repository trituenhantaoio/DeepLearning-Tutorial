{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Mạng nơ ron trong Pytorch - trituenhantao.io",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trituenhantaoio/DeepLearning-Tutorial/blob/master/M%E1%BA%A1ng_n%C6%A1_ron_trong_Pytorch_trituenhantao_io.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUt5p2vpnCro"
      },
      "source": [
        "\n",
        "Mạng nơ ron\n",
        "===============\n",
        "\n",
        "Mạng nơ ron có thể được xây dựng thông qua gói ``torch.nn``.\n",
        "\n",
        "Trong bài trước, bạn đã học về ``autograd``, ``torch.nn`` dựa trên\n",
        "``autograd`` để định nghĩa mô hình và tính đạo hàm.\n",
        "\n",
        "Ví dụ, mạng nơ ron dưới đây phân loại các chữ số:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/trituenhantaoio/DeepLearning-Tutorial/master/figures/mnist.png' alt='convnet'/>\n",
        "\n",
        "Mạng nơ ron này nhận đầu vào ``input``, truyền nó qua các tầng và đưa ra kết quả ``output``.\n",
        "\n",
        "Một quá trình chung để huấn luyện mạng nơ ron như sau:\n",
        "\n",
        "- Định nghĩa một mạng nơ ron với các tham số (hoặc trọng số) có thể được học trong quá trình huấn luyện.\n",
        "- Lặp qua một dataset chứa các ``input``\n",
        "- Xử lý ``input`` qua mạng nơ ron\n",
        "- Tính giá trị ``loss`` (``output`` sai khác bao nhiêu so với giá trị đúng)\n",
        "- Lan truyền ngược độ dốc để cập nhật các trọng số\n",
        "- Cập nhật trọng số của mạng với quy tắc đơn giản:\n",
        "  ``weight = weight - learning_rate * gradient``\n",
        "\n",
        "Định nghĩa mạng nơ ron\n",
        "------------------\n",
        "\n",
        "Chúng ta sẽ bắt đầu định nghĩa mạng nơ ron\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pNBC1qAf8zD",
        "outputId": "0fc3b374-0a2a-4e64-bf28-484770f23dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension \n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKpqpneGf8zN"
      },
      "source": [
        "Chúng ta vừa định nghĩa hàm ``forward``. Hàm ``backward``\n",
        "chứa các giá trị đạo hàm sẽ được tự động tính với ``autograd``.\n",
        "Chúng ta có thể dùng bất kỳ phép toán trên Tensor nào với hàm ``forward``.\n",
        "\n",
        "Các tham số của mạng có thể được trả về thông qua ``net.parameters()``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydW4C60nf8zO",
        "outputId": "fe5a8236-7e8a-47e0-f849-42a6be476daf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQCllFK1f8zT"
      },
      "source": [
        "Hãy thử với ``input`` ngẫu nhiên dạng 32x32.\n",
        "Chú ý: kích thước ``input`` cần thiết của mạng này (LeNet) là 32x32. Để có thể sử dụng mạng này trên dataset MNIST, ta cần định hình lại các tấm ảnh về 32x32.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kWmLld3f8zU",
        "outputId": "affee1ea-cdfd-470c-b580-3bb9f91dd4c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0119,  0.0622,  0.0178,  0.0636, -0.0043, -0.1265, -0.0876, -0.0235,\n",
            "          0.1200,  0.0494]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCADLBX7f8za"
      },
      "source": [
        "Thiết lập lại bộ đệm gradient của các trọng số về 0 và tính lan truyền ngược với giá trị gradient ngẫu nhiên:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4872cjgf8zb"
      },
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJdb_QLYf8zh"
      },
      "source": [
        "Toàn bộ gói ``torch.nn`` chỉ hỗ trợ ``input`` dưới dạng mini-batch chứ không hỗ trợ một mẫu riêng lẻ.\n",
        "\n",
        "    Ví dụ, ``nn.Conv2d`` sẽ nhận vào một Tensor 4D với các chiều là\n",
        "    ``nSamples x nChannels x Height x Width``.\n",
        "\n",
        "    Nếu ta chỉ có một mẫu thì sử dụng ``input.unsqueeze(0)`` để tạo ra một chiều giả.\n",
        "\n",
        "Trước khi học tiếp, hãy ôn lại một chút các bài đã học.\n",
        "\n",
        "**Ôn tập:**\n",
        "  -  ``torch.Tensor`` - Một *mảng đa chiều* hỗ trợ phép toán autograd\n",
        "     như ``backward()``.\n",
        "  -  ``nn.Module`` - Cấu phần giúp tạo mạng nơ ron *Một cách đơn giản để gói các trọng số*, hỗ trợ đưa lên GPU và các hoạt động liên quan.\n",
        "  -  ``nn.Parameter`` - Một loại Tensor, được coi là tham số khi trở thành một thuộc tính của một ``Module``.\n",
        "  -  ``autograd.Function`` - Định nghĩa phương thức lan truyền và lan truyền ngược một cách tự động. Mọi phương thức trên ``Tensor`` sẽ tạo ra một nút ``Function`` kết nối ``Tensor`` với hàm tạo ra nó và lưu trữ lịch sử tính toán.\n",
        "\n",
        "**Như vậy chúng ta đã có thể:**\n",
        "  -  Định nghĩa mạng nơ ron\n",
        "  -  Xử lý luồng thông tin từ ``input`` và tiến hành lan truyền ngược.\n",
        "\n",
        "**Các kiến thức mới:**\n",
        "  -  Tính toán ``loss``\n",
        "  -  Cập nhật trọng số mạng\n",
        "\n",
        "Hàm Loss\n",
        "-------------\n",
        "Hàm loss nhận đầu vào là cặp (output, target), và tính xem khoảng cách giữa ``output`` và ``target`` là bao xa.\n",
        "\n",
        "Có một số các\n",
        "[hàm loss](https://pytorch.org/docs/nn.html#loss-functions) trong gói ``nn``.\n",
        "Hàm loss đơn giản: ``nn.MSELoss`` sẽ tính sai số toàn phương trung bình giữa chúng.\n",
        "\n",
        "Ví dụ:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q3Yf518f8zi",
        "outputId": "b3dfd023-9f2d-4e27-bd99-2165242b0374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9714, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUY6bZErf8zn"
      },
      "source": [
        "Nếu theo dõi biến ``loss``, ta có thể thấy một biểu đồ tính toán như sau:\n",
        "\n",
        "\n",
        "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "          -> view -> linear -> relu -> linear -> relu -> linear\n",
        "          -> MSELoss\n",
        "          -> loss\n",
        "\n",
        "Khi ta gọi ``loss.backward()``, toàn bộ biểu đồ được tính đạo hàm, tức là ``loss`` và các Tensor trong biểu đồ có thuộc tính ``requires_grad=True``.\n",
        "\n",
        "Một cách trực quan hãy cùng theo dõi một số bước lan truyền ngược:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SE7B2Unf8zo",
        "outputId": "e3f53b72-487f-44d3-aa98-3452137e8e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7fe305d67b00>\n",
            "<AddmmBackward object at 0x7fe305d67c18>\n",
            "<AccumulateGrad object at 0x7fe305d67b00>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHZ8xfVf8zs"
      },
      "source": [
        "Lan truyền ngược\n",
        "--------\n",
        "Để lan truyền ngược lỗi, tất cả những gì chúng ta phải làm là gọi ``loss.backward()``.\n",
        "Mặc dù vậy, chúng ta cần xóa các giá trị gradient đã có để tránh việc gradient bị cộng dồn.\n",
        "\n",
        "\n",
        "Chúng ta sẽ gọi ``loss.backward()``, và xem giá trị bias của conv1 trước và sau khi gọi phương thức này.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS3yaOgMf8zu",
        "outputId": "71c987a9-c32a-4502-ca63-43d8f0940992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0108,  0.0025, -0.0007, -0.0046, -0.0062,  0.0010])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz-nwkGkf8zx"
      },
      "source": [
        "Bây giờ chúng ta sẽ xem cách sử dụng hàm loss.\n",
        "\n",
        "**Đọc thêm:**\n",
        "\n",
        "  Gói về mạng nơ ron chứa nhiều cấu phần và các hàm loss khác nhau giúp tạo nên các khối của mạng học sâu. Bạn có thể đọc đầy đủ tài liệu tại [đây](https://pytorch.org/docs/nn).\n",
        "\n",
        "**Điều cuối cùng cần học trong bài:**\n",
        "\n",
        "  - Cập nhật trọng số của mạng\n",
        "\n",
        "Cập nhật trọng số\n",
        "------------------\n",
        "Quy tắc cập nhật đơn giản nhất là sử dụng Stochastic Gradient\n",
        "Descent (SGD):\n",
        "\n",
        "     weight = weight - learning_rate * gradient\n",
        "\n",
        "Chúng ta có thể sử dụng Python code như sau:\n",
        "\n",
        "    learning_rate = 0.01\n",
        "    for f in net.parameters():\n",
        "        f.data.sub_(f.grad.data * learning_rate)\n",
        "\n",
        "Mặc dù vậy, đối với mạng nơ ron, chúng ta có thể sử dụng nhiều phương pháp cập nhật trọng số khác nhau như SGD, Nesterov-SGD, Adam, RMSProp, v..v..\n",
        "Để làm được điều này, chúng ta sử dụng một gói mang tên ``torch.optim`` cài đặt các phương pháp trên. Cách sử dụng rất đơn giản:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGnTsWXTf8zx"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI5i2vWgf8z1"
      },
      "source": [
        "Lưu ý:\n",
        "\n",
        "Chúng ta có thể thấy ``optimizer.zero_grad()`` cần được sử dụng trong quá trình cập nhật trọng số. Điều này là vì độ dốc được cộng dồn như đã giải thích trong phần `Lan truyền ngược`.\n"
      ]
    }
  ]
}